{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qh3TYwGwX0V"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "pnN-ALbkxpZb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text"
      ],
      "metadata": {
        "id": "3EF4v4IExqGh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"wonderland.txt\", \"w\", encoding=\"utf-8\").write(content)"
      ],
      "metadata": {
        "id": "Oaq3VzHxyl0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "8kG_bMKSy_OB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = \"/content/wonderland.txt\"\n",
        "text = open(FILE_PATH, encoding=\"utf-8\").read()\n"
      ],
      "metadata": {
        "id": "puP4xH2szBfY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.lower()"
      ],
      "metadata": {
        "id": "PIn2RlGuzT_E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
      ],
      "metadata": {
        "id": "OxENd-Xez9I_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(text)\n",
        "n_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHNtfzdK0Dtb",
        "outputId": "9f795390-d66d-48e0-a810-dc56890f3c5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158225"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=''.join(sorted(set(text)))\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TepeP4g60BF4",
        "outputId": "6eedd44c-f95f-4121-aa75-c9f8a76517de"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n 0123456789abcdefghijklmnopqrstuvwxyzù—‘’“”•™\\ufeff'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_unique_chars = len(vocab)\n",
        "n_unique_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6Oqj0sz0dO9",
        "outputId": "7a486502-1259-4bd4-8433-3ef99a119f89"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary that converts characters to integers\n",
        "char2int ={c:i for i ,c in enumerate (vocab)}\n",
        "char2int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glu2KBBX0tLE",
        "outputId": "77757062-43e1-4f4d-de5a-226735aab971"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '0': 2,\n",
              " '1': 3,\n",
              " '2': 4,\n",
              " '3': 5,\n",
              " '4': 6,\n",
              " '5': 7,\n",
              " '6': 8,\n",
              " '7': 9,\n",
              " '8': 10,\n",
              " '9': 11,\n",
              " 'a': 12,\n",
              " 'b': 13,\n",
              " 'c': 14,\n",
              " 'd': 15,\n",
              " 'e': 16,\n",
              " 'f': 17,\n",
              " 'g': 18,\n",
              " 'h': 19,\n",
              " 'i': 20,\n",
              " 'j': 21,\n",
              " 'k': 22,\n",
              " 'l': 23,\n",
              " 'm': 24,\n",
              " 'n': 25,\n",
              " 'o': 26,\n",
              " 'p': 27,\n",
              " 'q': 28,\n",
              " 'r': 29,\n",
              " 's': 30,\n",
              " 't': 31,\n",
              " 'u': 32,\n",
              " 'v': 33,\n",
              " 'w': 34,\n",
              " 'x': 35,\n",
              " 'y': 36,\n",
              " 'z': 37,\n",
              " 'ù': 38,\n",
              " '—': 39,\n",
              " '‘': 40,\n",
              " '’': 41,\n",
              " '“': 42,\n",
              " '”': 43,\n",
              " '•': 44,\n",
              " '™': 45,\n",
              " '\\ufeff': 46}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary that converts integers to characters\n",
        "int2char={i : c for i ,c in enumerate (vocab)}\n",
        "int2char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40RLG2My1DuV",
        "outputId": "28bf11cf-77fe-400c-fb07-f06f55835841"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '0',\n",
              " 3: '1',\n",
              " 4: '2',\n",
              " 5: '3',\n",
              " 6: '4',\n",
              " 7: '5',\n",
              " 8: '6',\n",
              " 9: '7',\n",
              " 10: '8',\n",
              " 11: '9',\n",
              " 12: 'a',\n",
              " 13: 'b',\n",
              " 14: 'c',\n",
              " 15: 'd',\n",
              " 16: 'e',\n",
              " 17: 'f',\n",
              " 18: 'g',\n",
              " 19: 'h',\n",
              " 20: 'i',\n",
              " 21: 'j',\n",
              " 22: 'k',\n",
              " 23: 'l',\n",
              " 24: 'm',\n",
              " 25: 'n',\n",
              " 26: 'o',\n",
              " 27: 'p',\n",
              " 28: 'q',\n",
              " 29: 'r',\n",
              " 30: 's',\n",
              " 31: 't',\n",
              " 32: 'u',\n",
              " 33: 'v',\n",
              " 34: 'w',\n",
              " 35: 'x',\n",
              " 36: 'y',\n",
              " 37: 'z',\n",
              " 38: 'ù',\n",
              " 39: '—',\n",
              " 40: '‘',\n",
              " 41: '’',\n",
              " 42: '“',\n",
              " 43: '”',\n",
              " 44: '•',\n",
              " 45: '™',\n",
              " 46: '\\ufeff'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all text into integers\n",
        "encoded_text=np.array([char2int[c]for c in text])\n",
        "encoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QgDbG441Sb5",
        "outputId": "f60391b4-d9ea-437b-b195-5add5cf65b21"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([46, 31, 19, ...,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct tf.data.Dataset object\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "char_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWt8QsP011iH",
        "outputId": "98b7026e-9eae-40ff-b9a6-eaf749a81762"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 5 characters\n",
        "for char in char_dataset.take(8):\n",
        "  print(char.numpy(),int2char[char.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izGC9oZ12TnF",
        "outputId": "3650af65-aafa-4832-96f9-2c78d273f72b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46 ﻿\n",
            "31 t\n",
            "19 h\n",
            "16 e\n",
            "1  \n",
            "27 p\n",
            "29 r\n",
            "26 o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "#تقطيع النص كله الي بيانات مقطعة\n",
        "# build sequences by batching\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "sequences"
      ],
      "metadata": {
        "id": "UgVRlVQ427GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sequence in sequences.take(3):\n",
        "  print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OmMBHlb3548",
        "outputId": "285704a4-e7ef-4c63-ce02-03f39f6c74b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿the project gutenberg ebook of alices adventures in wonderland\n",
            "    \n",
            "this ebook is for the use of anyone anywhere in the united states and\n",
            "most other parts of the world at no cost and with almost no re\n",
            "strictions\n",
            "whatsoever you may copy it give it away or reuse it under the terms\n",
            "of the project gutenberg license included with this ebook or online\n",
            "at wwwgutenbergorg if you are not located in the unite\n",
            "d states\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this ebook\n",
            "\n",
            "title alices adventures in wonderland\n",
            "\n",
            "\n",
            "author lewis carroll\n",
            "\n",
            "release date june 27 2008 ebook 11\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sample(sample):\n",
        "      ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "      for i in range(1, (len(sample)-1) // 2):\n",
        "        input_ = sample[i: i+sequence_length]\n",
        "        target = sample[i+sequence_length]\n",
        "        # extend the dataset with these samples by concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "      return ds\n",
        "# prepare inputs and targets\n",
        "dataset = sequences.flat_map(split_sample)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WelAQ_Dc4Sd0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_samples(input_, target):\n",
        "    # onehot encode the inputs and the targets\n",
        "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)"
      ],
      "metadata": {
        "id": "eFZvKNLL6LsM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "zyjqvz5S6bH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 2 samples\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xizuQI2x6l_P",
        "outputId": "7c566da4-6468-4d3f-aab7-3367bf40d0e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: ﻿the project gutenberg ebook of alices adventures in wonderland\n",
            "    \n",
            "this ebook is for the use of an\n",
            "Target: y\n",
            "Input shape: (100, 47)\n",
            "Target shape: (47,)\n",
            "================================================== \n",
            "\n",
            "Input: the project gutenberg ebook of alices adventures in wonderland\n",
            "    \n",
            "this ebook is for the use of any\n",
            "Target: o\n",
            "Input shape: (100, 47)\n",
            "Target shape: (47,)\n",
            "================================================== \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#عمل خلط للبيانات و تقسيمها الي باشتات\n",
        "# repeat, shuffle and batch the dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "X7_TZNKx66cP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#بناء الموديل\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),\n",
        "])"
      ],
      "metadata": {
        "id": "DUN8hTLK6_If"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SfjHPz67KUB",
        "outputId": "55c25853-4f76-4ca6-fa83-e9029ea3ddc5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1235/1235 [==============================] - 1531s 1s/step - loss: 1.8008\n",
            "Epoch 2/10\n",
            "1235/1235 [==============================] - 1519s 1s/step - loss: 1.5839\n",
            "Epoch 3/10\n",
            "1235/1235 [==============================] - 1525s 1s/step - loss: 1.4250\n",
            "Epoch 4/10\n",
            "1235/1235 [==============================] - 1516s 1s/step - loss: 1.2912\n",
            "Epoch 5/10\n",
            "1235/1235 [==============================] - 1516s 1s/step - loss: 1.1762\n",
            "Epoch 6/10\n",
            "1235/1235 [==============================] - 1517s 1s/step - loss: 1.0719\n",
            "Epoch 7/10\n",
            "1235/1235 [==============================] - 1500s 1s/step - loss: 0.9810\n",
            "Epoch 8/10\n",
            "1235/1235 [==============================] - 1498s 1s/step - loss: 0.8982\n",
            "Epoch 9/10\n",
            "1235/1235 [==============================] - 1503s 1s/step - loss: 0.8231\n",
            "Epoch 10/10\n",
            "1235/1235 [==============================] - 1507s 1s/step - loss: 0.7618\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e1322be27a0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"git+https://github.com/tqdm/tqdm.git@devel#egg=tqdm\"\n"
      ],
      "metadata": {
        "id": "QZpJQFN2Bwyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "seed = \"chapter xiii\"\n",
        "vocab_size = len(char2int)\n",
        "s = seed\n",
        "n_chars = 10000\n",
        "# generate 400 characters\n",
        "generated = \"\"\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
        "    # make the input sequence\n",
        "    X = np.zeros((1, sequence_length, vocab_size))\n",
        "    for t, char in enumerate(seed):\n",
        "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "    # predict the next character\n",
        "    predicted = model.predict(X, verbose=0)[0]\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(predicted)\n",
        "    # converting the integer to a character\n",
        "    next_char = int2char[next_index]\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", s)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztxui_4X6_tB",
        "outputId": "512098c9-723d-4509-b42f-35ca54ed66ce"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating text: 100%|██████████| 10000/10000 [10:16<00:00, 16.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: chapter xiii\n",
            "Generated text:\n",
            "d to grin” said the whole party go do little thing” she said to get through they were indeed you should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” she was getting of copyright law do i think—” thought she knever my deep with the books to get thank i should think” and she went of the wides—” s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}