{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahmoudmagdyhassan/NLP-and-CV/blob/main/text_classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dARub7id2NJa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fif9Crzw2NJc"
      },
      "source": [
        "## Load the data: IMDB movie review sentiment classification\n",
        "\n",
        "Let's download the data and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i6pJE4p2NJd"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KRgzV7L2NJe"
      },
      "source": [
        "The `aclImdb` folder contains a `train` and `test` subfolder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMn0ppfT2NJe",
        "outputId": "fa8dcff7-8f41-4936-a162-97d3177b2a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIULykpn2NJg",
        "outputId": "5160a639-fbf8-4c24-fa1d-ee40ba398f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6ESYz7O2NJh",
        "outputId": "b2be66f6-321f-4496-8de9-deba086ab1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  unsup  unsupBow.feat  urls_neg.txt  urls_pos.txt  urls_unsup.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDgo_QOg2NJi"
      },
      "source": [
        "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n",
        " which represents one review (either positive or negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLhe8LCO2NJj"
      },
      "outputs": [],
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgGiTn9i2NJk"
      },
      "source": [
        "We are only interested in the `pos` and `neg` subfolders, so let's delete the other subfolder that has text files in it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Afs11E1F2NJl"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrkSxVE42NJn",
        "outputId": "770a8f84-4a81-49ba-c72e-998c95adddc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfrCIWad2NJn"
      },
      "source": [
        "Let's preview a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ-ZytnX2NJo",
        "outputId": "7a4de9f5-2f11-4ff2-f883-ae326b59020e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'I rented this horrible movie. The worst think I have ever seen. I believe a 1st grade class could have done a better job. The worse film I have ever seen and I have seen some bad ones. Nothing scary except I paid 1.50 to rent it and that was 1.49 too much. The acting is horrible, the characters are worse and the film is just a piece of trash. The slauther house scenes are so low budget that it makes a B movied look like an Oscar candidate. All I can say is if you wnat to waste a good evening and a little money go rent this horrible flick. I would rather watch killer clowns from outer space while sitting in a bucket of razors than sit through this flop again'\n",
            "0\n",
            "b\"I spent almost two hours watching a movie that I thought, with all the good actors in it, would be worth watching. I couldn't believe it when the movie ended and I had absolutely no idea what had happened.....I was mad because I could have used that time doing something else....I tried to figure it all out, but really had no clue. Thanks to those who figured it out and have explained it....right or wrong, it's better than not knowing anything!! Who was the lady in the movie with dark hair that we saw a couple of times driving away? How did First Lady know that her husband was cheating on her? At the end of the movie Kate said she would eventually find out the truth. Does this mean that we're going to be subjected to End Game 2?\"\n",
            "0\n",
            "b\"As incredible as it may seem, Gojoe is an anime- and Hong Kong-inspired samurai action flick with a pacifistic message. This ankle of the film is effectively portrayed through the protagonist (a great acting job done by Daisuke Ryu), a killer-turned-to-boddhist-monk Benkei. Benkei has sworn never to kill again, but he still takes up the sword to fight what he thinks is a demon invasion...<br /><br />Gojoe is a film difficult to rate. It's visual imagery is stunningly crafted and beautiful, but it uses too much trickery (circling camera and high speed drives, expressionistic shots, leeched colors, digital effects etc.), so the end result is somewhat tiring. That said, the beginning and the ending of the film are nevertheless both elegant and powerful. If only the director Sogo Ishii would have been wise enough not to overuse his bag of tricks.<br /><br />Other problem with Gojoe is the amount of violence. For a film with such an anti-violent message Gojoe wastes way too much energy and screen time to depict the endless battle scenes. Also, the way the violence is shown is always on the edge of being self-indulgent; in fact, a blood shower against the night sky seems to be one of the films signature images. Luckily, Ishii is wise enough to show the ugly, tragic side of violence as well. Still, it seems that Ishii is not sure whether he's making a traditional action film or a deeply moral allegory. The audience can't be sure of this, either, until the very end of the film. The powerful (albeit cynical) ending is what saves Gojoe; it clearly emphasizes that this film is something more than a mere gore-fest.\"\n",
            "1\n",
            "b\"I consider this film one of the worst in the Nightmare series. It was so boring that I couldn't remember a thing 20 minutes after the film was over, it even tires me to write a review on it.<br /><br />Okay, #4 was a joke and Freddy was the joker. #5 tried to return to the roots of the series. It was darker and more atmospheric than Nightmare 4, which is a good thing, basically. They tried to shoot a horror film instead of a comedy. Unfortunately they forgot to add suspense and scares. Because of that Nightmare 5: The Dream Child is neither funny nor is it scary. What we actually get is a boring film with the usual bad actors (maybe with the exception of Lisa Wilcox).<br /><br />The plot (Freddy killing Lisa's friends by using the dreams of Lisa's unborn child) has a good base but it just isn't enough for 90 minutes of film. Sometimes the story gets very confusing (maybe because there isn't any) and you can't stop wondering what the filmmakers were aiming at. The screenplay must have had more holes than Swiss Cheese and the film therefore was very cheesy itself (let me say that I don't like cheese though, even if I am from Switzerland). Not even the special effects were as good as for example in part 4.<br /><br />Don't bother to rent/buy this film if not for completeness, it's quite a mess.<br /><br />My rating: 4/10 (get used to it, #6 is also a messy one...)\"\n",
            "0\n",
            "b'This is one of the best films we watched in my high school Spanish class. If you are a fan of the opera, this film will strongly entertain you. Of course, the dancing is wonderful. Watching these amazing dancers moving to the music of Bizet is well worth checking out.'\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNCNGIK92NJp"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "In particular, we remove `<br />` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TbrdmYFh2NJp"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26blrKKg6cLk",
        "outputId": "d3de8293-cd79-4bec-e6f4-5532a836f522"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O9rj_Ap2NJq"
      },
      "source": [
        "## Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:\n",
        "\n",
        "**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n",
        " strings, like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5WDaEzF2NJq"
      },
      "source": [
        "```python\n",
        "text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```\n",
        "\n",
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
        " feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two is that option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training on GPU.\n",
        "So if you're training the model on GPU, you probably want to go with this option to get\n",
        " the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw\n",
        "strings as input, like in the code snippet for option 1 above. This can be done after\n",
        " training. We do this in the last section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yxGeOC1G2NJr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC61qshX6zcV",
        "outputId": "597c5839-9788-45e2-cb2b-5142de291d5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 500), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmNnGlVS2NJr"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We choose a simple 1D convnet starting with an `Embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Iuksj-RN2NJr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.BatchNormalization()(x)  # Add Batch Normalization\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.BatchNormalization()(x)  # Add Batch Normalization\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Add Batch Normalization before the final Dense layer\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMnm7_uu2NJs"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfdUeKh22NJs",
        "outputId": "7b0ffad0-1bba-458c-a26d-784707b7aaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "625/625 [==============================] - 54s 81ms/step - loss: 0.6884 - accuracy: 0.5794 - val_loss: 0.4376 - val_accuracy: 0.8244\n",
            "Epoch 2/15\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3344 - accuracy: 0.8600 - val_loss: 0.3030 - val_accuracy: 0.8738\n",
            "Epoch 3/15\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1941 - accuracy: 0.9253 - val_loss: 0.3637 - val_accuracy: 0.8630\n",
            "Epoch 4/15\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1189 - accuracy: 0.9568 - val_loss: 0.4009 - val_accuracy: 0.8744\n",
            "Epoch 5/15\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.0850 - accuracy: 0.9696 - val_loss: 0.4145 - val_accuracy: 0.8722\n",
            "Epoch 6/15\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.0590 - accuracy: 0.9803 - val_loss: 0.5607 - val_accuracy: 0.8584\n",
            "Epoch 7/15\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.5239 - val_accuracy: 0.8634\n",
            "Epoch 8/15\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0439 - accuracy: 0.9844 - val_loss: 0.5371 - val_accuracy: 0.8606\n",
            "Epoch 9/15\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.0398 - accuracy: 0.9857 - val_loss: 0.6022 - val_accuracy: 0.8548\n",
            "Epoch 10/15\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0358 - accuracy: 0.9876 - val_loss: 0.6222 - val_accuracy: 0.8618\n",
            "Epoch 11/15\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.0322 - accuracy: 0.9883 - val_loss: 0.6137 - val_accuracy: 0.8630\n",
            "Epoch 12/15\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.6426 - val_accuracy: 0.8582\n",
            "Epoch 13/15\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.0271 - accuracy: 0.9912 - val_loss: 0.6296 - val_accuracy: 0.8616\n",
            "Epoch 14/15\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.0289 - accuracy: 0.9901 - val_loss: 0.6490 - val_accuracy: 0.8574\n",
            "Epoch 15/15\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.5815 - val_accuracy: 0.8660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bf878c108e0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "epochs = 15\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1rAD5Ie2NJt"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmm8OVyY2NJt",
        "outputId": "33bd961b-b25e-414a-82c5-9290d17c9f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6307 - accuracy: 0.8546\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6307352185249329, 0.854640007019043]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ZptYy02NJt"
      },
      "source": [
        "## Make an end-to-end model\n",
        "\n",
        "If you want to obtain a model capable of processing raw strings, you can simply\n",
        "create a new model (using the weights we just trained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfeYJ8ou2NJt"
      },
      "outputs": [],
      "source": [
        "# A string input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_from_scratch",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}