{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoo89t7Pexf+aiAraxKywu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahmoudmagdyhassan/NLP-and-CV/blob/main/parking_spots_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import os\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as k\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
      ],
      "metadata": {
        "id": "GKgjGDv7UoBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_train = 0\n",
        "files_validation = 0\n",
        "\n",
        "cwd = os.getcwd()\n",
        "folder = '/content/Deep-Learning/parking_spots_detector/train_data/train'\n",
        "for sub_folder in os.listdir(folder):\n",
        "    path, dirs, files = next(os.walk(os.path.join(folder,sub_folder)))\n",
        "    files_train += len(files)\n",
        "\n",
        "\n",
        "folder = '/content/Deep-Learning/parking_spots_detector/train_data/test'\n",
        "for sub_folder in os.listdir(folder):\n",
        "    path, dirs, files = next(os.walk(os.path.join(folder,sub_folder)))\n",
        "    files_validation += len(files)\n",
        "\n",
        "print(files_train,files_validation)"
      ],
      "metadata": {
        "id": "aHeUUl12U4KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 48, 48\n",
        "train_data_dir = \"/content/Deep-Learning/parking_spots_detector/train_data/train\"\n",
        "validation_data_dir = \"/content/Deep-Learning/parking_spots_detector/train_data/test\"\n",
        "nb_train_samples = files_train\n",
        "nb_validation_samples = files_validation\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "num_classes = 2"
      ],
      "metadata": {
        "id": "-x0BE9EFVUrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "\n",
        "# creating the final model\n",
        "model_final = Model(model.input, predictions)\n",
        "\n",
        "\n",
        "model_final.compile(optimizer='adam',\n",
        "              loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "model_final.summary()"
      ],
      "metadata": {
        "id": "dKKUS8y-VmsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the train and test generators with data Augumentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "rescale = 1./255,\n",
        "horizontal_flip = True,\n",
        "fill_mode = \"nearest\",\n",
        "zoom_range = 0.1,\n",
        "width_shift_range = 0.1,\n",
        "height_shift_range=0.1,\n",
        "rotation_range=5)\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "rescale = 1./255,\n",
        "horizontal_flip = True,\n",
        "fill_mode = \"nearest\",\n",
        "zoom_range = 0.1,\n",
        "width_shift_range = 0.1,\n",
        "height_shift_range=0.1,\n",
        "rotation_range=5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "batch_size = batch_size,\n",
        "class_mode = \"categorical\")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "validation_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "class_mode = \"categorical\")"
      ],
      "metadata": {
        "id": "WpLheRwrV5Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model according to the conditions\n",
        "checkpoint = ModelCheckpoint(\"car1.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "### Start training!\n",
        "\n",
        "history_object = model_final.fit_generator(\n",
        "train_generator,\n",
        "epochs = 15,\n",
        "validation_data = validation_generator,\n",
        "callbacks = [checkpoint, early])\n",
        "\n"
      ],
      "metadata": {
        "id": "5MzZ7MjnWF44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(history_object.history.keys())\n",
        "plt.plot(history_object.history['accuracy'])\n",
        "plt.plot(history_object.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history_object.history['loss'])\n",
        "plt.plot(history_object.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MLdHf8Mycail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "cwd = os.getcwd()\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "\n",
        "def show_images(images, cmap=None):\n",
        "    cols = 2\n",
        "    rows = (len(images)+1)//cols\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        # use gray scale color map if there is only one channel\n",
        "        cmap = 'gray' if len(image.shape)==2 else cmap\n",
        "        plt.imshow(image, cmap=cmap)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "test_images = [plt.imread(path) for path in glob.glob('/content/Deep-Learning/parking_spots_detector/test_images/*.jpg')]\n",
        "\n",
        "show_images(test_images)\n"
      ],
      "metadata": {
        "id": "T3GpXdSqcBDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image is expected be in RGB color space# image\n",
        "def select_rgb_white_yellow(image):\n",
        "    # white color mask\n",
        "    lower = np.uint8([120, 120, 120])\n",
        "    upper = np.uint8([255, 255, 255])\n",
        "    white_mask = cv2.inRange(image, lower, upper)\n",
        "    # yellow color mask\n",
        "    lower = np.uint8([190, 190,   0])\n",
        "    upper = np.uint8([255, 255, 255])\n",
        "    yellow_mask = cv2.inRange(image, lower, upper)\n",
        "    # combine the mask\n",
        "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "    masked = cv2.bitwise_and(image, image, mask = mask)\n",
        "    return masked\n",
        "\n",
        "white_yellow_images = list(map(select_rgb_white_yellow, test_images))\n",
        "show_images(white_yellow_images)\n"
      ],
      "metadata": {
        "id": "pzTl1TNrfJ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_gray_scale(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "gray_images = list(map(convert_gray_scale, white_yellow_images))\n",
        "\n",
        "show_images(gray_images)"
      ],
      "metadata": {
        "id": "BLMMACN5fWY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_edges(image, low_threshold=50, high_threshold=200):\n",
        "    return cv2.Canny(image, low_threshold, high_threshold)\n",
        "\n",
        "edge_images = list(map(lambda image: detect_edges(image), gray_images))\n",
        "\n",
        "show_images(edge_images)\n"
      ],
      "metadata": {
        "id": "op2mCPpTfa1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_region(image, vertices):\n",
        "    \"\"\"\n",
        "    Create the mask using the vertices and apply it to the input image\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(image)\n",
        "    if len(mask.shape)==2:\n",
        "        cv2.fillPoly(mask, vertices, 255)\n",
        "    else:\n",
        "        cv2.fillPoly(mask, vertices, (255,)*mask.shape[2]) # in case, the input image has a channel dimension\n",
        "    return cv2.bitwise_and(image, mask)\n",
        "\n",
        "\n",
        "def select_region(image):\n",
        "    \"\"\"\n",
        "    It keeps the region surrounded by the `vertices` (i.e. polygon).  Other area is set to 0 (black).\n",
        "    \"\"\"\n",
        "    # first, define the polygon by vertices\n",
        "    rows, cols = image.shape[:2]\n",
        "    print(rows)\n",
        "    print(cols)\n",
        "    pt_1  = [cols*0.005, rows*0.90]\n",
        "    pt_2 = [cols*0.05, rows*0.70]\n",
        "    pt_3 = [cols*0.30, rows*0.55]\n",
        "    pt_4 = [cols*0.6, rows*0.15]\n",
        "    pt_5 = [cols*0.90, rows*0.15]\n",
        "    pt_6 = [cols*0.90, rows*0.90]\n",
        "\n",
        "\n",
        "    # the vertices are an array of polygons (i.e array of arrays) and the data type must be integer\n",
        "    vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5,pt_6]], dtype=np.int32)\n",
        "    return filter_region(image, vertices)\n",
        "\n",
        "\n",
        "# images showing the region of interest only\n",
        "roi_images = list(map(select_region, edge_images))\n",
        "\n",
        "show_images(roi_images)"
      ],
      "metadata": {
        "id": "bISLy2j_fjJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hough_lines(image):\n",
        "    \"\"\"\n",
        "    `image` should be the output of a Canny transform.\n",
        "\n",
        "    Returns hough lines (not the image with lines)\n",
        "    \"\"\"\n",
        "    return cv2.HoughLinesP(image, rho=0.1, theta=np.pi/20, threshold=20, minLineLength=10, maxLineGap=4)\n",
        "\n",
        "\n",
        "list_of_lines = list(map(hough_lines, roi_images))\n",
        "def draw_lines(image, lines, color=[255, 0, 0], thickness=2, make_copy=True):\n",
        "    # the lines returned by cv2.HoughLinesP has the shape (-1, 1, 4)\n",
        "    if make_copy:\n",
        "        image = np.copy(image) # don't want to modify the original\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        for x1,y1,x2,y2 in line:\n",
        "            if abs(y2-y1) <=1 and abs(x2-x1) >=25 and abs(x2-x1) <= 55:\n",
        "                cleaned.append((x1,y1,x2,y2))\n",
        "                cv2.line(image, (x1, y1), (x2, y2), color, thickness)\n",
        "    print(\" No lines detected: \", len(cleaned))\n",
        "    return image\n",
        "\n",
        "\n",
        "line_images = []\n",
        "for image, lines in zip(test_images, list_of_lines):\n",
        "    line_images.append(draw_lines(image, lines))\n",
        "\n",
        "show_images(line_images)"
      ],
      "metadata": {
        "id": "orvcXgCRfzi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function takes an image and a list of lines as input.\n",
        "The optional parameter make_copy is set to True by default, indicating whether to create a copy of the input image or not. A new image (new_image) is created if make_copy is True.\n",
        "\n",
        "\n",
        "\n",
        "This step cleans the input lines based on specific criteria. It filters out lines that don't meet certain conditions related to the vertical and horizontal distances between their endpoints.\n",
        "The filtered lines are stored in the cleaned list.\n",
        "\n",
        "\n",
        "\n",
        "The cleaned list is sorted based on the x-coordinate of the starting point (x1).\n",
        "The sorted list is stored in list1.\n",
        "The sorted list is printed for inspection.\n",
        "\n",
        "\n",
        "clusters: This dictionary will store the clusters of lines. The keys represent the index of the clusters, and the values are lists of lines in each cluster.\n",
        "\n",
        "dIndex: This variable is used to keep track of the current cluster index.\n",
        "\n",
        "clus_dist: This variable defines the maximum distance between consecutive lines to consider them part of the same cluster.\n",
        "\n",
        "The loop (for i in range(len(list1) - 1)) iterates through the sorted list of cleaned lines (list1).\n",
        "\n",
        "distance = abs(list1[i + 1][0] - list1[i][0]): This calculates the horizontal distance between consecutive lines.\n",
        "\n",
        "The if statement checks if the distance is less than or equal to the specified cluster distance (clus_dist).\n",
        "\n",
        "If the distance criterion is met, it adds the lines to the current cluster (clusters[dIndex]).\n",
        "\n",
        "If a new cluster is encountered (when the distance criterion is not met), it increments the cluster index (dIndex += 1), preparing for the next cluster.\n",
        "\n",
        "In summary, this loop processes the sorted list of lines and groups them into clusters based on their horizontal proximity (clus_dist). Each cluster is stored in the clusters dictionary. The result is a dictionary where each key corresponds to a cluster index, and the values are lists of lines within that cluster.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SpgSqwFEkVkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_blocks(image, lines, make_copy=True):\n",
        "    if make_copy:\n",
        "        new_image = np.copy(image)\n",
        "    #Step 1: Create a clean list of lines\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        for x1,y1,x2,y2 in line:\n",
        "            if abs(y2-y1) <=1 and abs(x2-x1) >=25 and abs(x2-x1) <= 55:\n",
        "                cleaned.append((x1,y1,x2,y2))\n",
        "\n",
        "    #Step 2: Sort cleaned by x1 position\n",
        "    import operator\n",
        "    list1 = sorted(cleaned, key=operator.itemgetter(0, 1))\n",
        "    print(list1)\n",
        "\n",
        "    #Step 3: Find clusters of x1 close together - clust_dist apart\n",
        "    clusters = {}\n",
        "    dIndex = 0\n",
        "    clus_dist = 10\n",
        "\n",
        "    for i in range(len(list1) - 1):\n",
        "        distance = abs(list1[i+1][0] - list1[i][0])\n",
        "    #         print(distance)\n",
        "        if distance <= clus_dist:\n",
        "            if not dIndex in clusters.keys(): clusters[dIndex] = []\n",
        "            clusters[dIndex].append(list1[i])\n",
        "            clusters[dIndex].append(list1[i + 1])\n",
        "\n",
        "        else:\n",
        "            dIndex += 1\n",
        "\n",
        "    #Step 4: Identify coordinates of rectangle around this cluster\n",
        "    rects = {}\n",
        "    i = 0\n",
        "    for key in clusters:\n",
        "        all_list = clusters[key]\n",
        "        cleaned = list(set(all_list))\n",
        "        if len(cleaned) > 5:\n",
        "            cleaned = sorted(cleaned, key=lambda tup: tup[1])\n",
        "            #print(\"cleaned\",cleaned)\n",
        "            avg_y1 = cleaned[0][1]\n",
        "            avg_y2 = cleaned[-1][1]\n",
        "            print(avg_y1, avg_y2)\n",
        "            avg_x1 = 0\n",
        "            avg_x2 = 0\n",
        "            for tup in cleaned:\n",
        "                avg_x1 += tup[0]\n",
        "                avg_x2 += tup[2]\n",
        "            avg_x1 = avg_x1/len(cleaned)\n",
        "            avg_x2 = avg_x2/len(cleaned)\n",
        "            rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2)\n",
        "            i += 1\n",
        "\n",
        "    print(\"Num Parking Lanes: \", len(rects))\n",
        "    #Step 5: Draw the rectangles on the image\n",
        "    buff = 7\n",
        "    for key in rects:\n",
        "        tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1]))\n",
        "        tup_botRight = (int(rects[key][2] + buff), int(rects[key][3]))\n",
        "#         print(tup_topLeft, tup_botRight)\n",
        "        cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3)\n",
        "    return new_image, rects\n",
        "\n",
        "# images showing the region of interest only\n",
        "rect_images = []\n",
        "rect_coords = []\n",
        "for image, lines in zip(test_images, list_of_lines):\n",
        "    new_image, rects = identify_blocks(image, lines)\n",
        "    rect_images.append(new_image)\n",
        "    rect_coords.append(rects)\n",
        "\n",
        "show_images(rect_images)"
      ],
      "metadata": {
        "id": "0qco-d5VgjAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def draw_parking(image, rects, make_copy = True, color=[255, 0, 0], thickness=2, save = True):\n",
        "    if make_copy:\n",
        "        new_image = np.copy(image)\n",
        "    gap = 15.5\n",
        "    spot_dict = {} # maps each parking ID to its coords\n",
        "    tot_spots = 0\n",
        "    adj_y1 = {0: -8, 1:-25, 2:10, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:15, 11:-32}\n",
        "    adj_y2 = {0: 5, 1: -10, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30}\n",
        "\n",
        "    adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0}\n",
        "    adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0}\n",
        "    for key in rects:\n",
        "        # Horizontal lines\n",
        "        tup = rects[key]\n",
        "        x1 = int(tup[0]+ adj_x1[key])\n",
        "        x2 = int(tup[2]+ adj_x2[key])\n",
        "        y1 = int(tup[1] + adj_y1[key])\n",
        "        y2 = int(tup[3] + adj_y2[key])\n",
        "        cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2)\n",
        "        num_splits = int(abs(y2-y1)//gap)\n",
        "        for i in range(0, num_splits+1):\n",
        "            y = int(y1 + i*gap)\n",
        "            cv2.line(new_image, (x1, y), (x2, y), color, thickness)\n",
        "        if key > 0 and key < len(rects) -1 :\n",
        "            #draw vertical lines\n",
        "            x = int((x1 + x2)/2)\n",
        "            cv2.line(new_image, (x, y1), (x, y2), color, thickness)\n",
        "        # Add up spots in this lane\n",
        "        if key == 0 or key == (len(rects) -1):\n",
        "            tot_spots += num_splits +1\n",
        "        else:\n",
        "            tot_spots += 2*(num_splits +1)\n",
        "\n",
        "        # Dictionary of spot positions\n",
        "        if key == 0 or key == (len(rects) -1):\n",
        "            for i in range(0, num_splits+1):\n",
        "                cur_len = len(spot_dict)\n",
        "                y = int(y1 + i*gap)\n",
        "                spot_dict[(x1, y, x2, y+gap)] = cur_len +1\n",
        "        else:\n",
        "            for i in range(0, num_splits+1):\n",
        "                cur_len = len(spot_dict)\n",
        "                y = int(y1 + i*gap)\n",
        "                x = int((x1 + x2)/2)\n",
        "                spot_dict[(x1, y, x, y+gap)] = cur_len +1\n",
        "                spot_dict[(x, y, x2, y+gap)] = cur_len +2\n",
        "\n",
        "    print(\"total parking spaces: \", tot_spots, cur_len)\n",
        "    if save:\n",
        "        filename = 'with_parking.jpg'\n",
        "        cv2.imwrite(filename, new_image)\n",
        "    return new_image, spot_dict\n",
        "\n",
        "delineated = []\n",
        "spot_pos = []\n",
        "for image, rects in zip(test_images, rect_coords):\n",
        "    new_image, spot_dict = draw_parking(image, rects)\n",
        "    delineated.append(new_image)\n",
        "    spot_pos.append(spot_dict)\n",
        "\n",
        "show_images(delineated)"
      ],
      "metadata": {
        "id": "6UN3HeosSLVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_spot_dict = spot_pos[1]\n",
        "print(len(final_spot_dict))"
      ],
      "metadata": {
        "id": "TBsn9cPwXfN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def assign_spots_map(image, spot_dict=final_spot_dict, make_copy = True, color=[255, 0, 0], thickness=2):\n",
        "    if make_copy:\n",
        "        new_image = np.copy(image)\n",
        "    for spot in spot_dict.keys():\n",
        "        (x1, y1, x2, y2) = spot\n",
        "        cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness)\n",
        "    return new_image\n",
        "\n",
        "marked_spot_images = list(map(assign_spots_map, test_images))\n",
        "show_images(marked_spot_images)\n"
      ],
      "metadata": {
        "id": "4lOUhSfGSW7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Save spot dictionary as pickle file\n",
        "import pickle\n",
        "\n",
        "with open('spot_dict.pickle', 'wb') as handle:\n",
        "    pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def save_images_for_cnn(image, spot_dict = final_spot_dict, folder_name ='for_cnn'):\n",
        "    for spot in spot_dict.keys():\n",
        "        (x1, y1, x2, y2) = spot\n",
        "        (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))\n",
        "        #crop this image\n",
        "#         print(image.shape)\n",
        "        spot_img = image[y1:y2, x1:x2]\n",
        "        spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0)\n",
        "        spot_id = spot_dict[spot]\n",
        "\n",
        "        filename = 'spot' + str(spot_id) +'.jpg'\n",
        "        print(spot_img.shape, filename, (x1,x2,y1,y2))\n",
        "\n",
        "        cv2.imwrite(os.path.join(folder_name, filename), spot_img)\n",
        "\n",
        "save_images_for_cnn(test_images[0])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "--UV0OBVUhHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports for making predictions\n",
        "from PIL import Image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "\n",
        "cwd = os.getcwd()\n",
        "top_model_weights_path = 'car1.h5'\n",
        "\n",
        "class_dictionary = {}\n",
        "class_dictionary[0] = 'empty'\n",
        "class_dictionary[1] = 'occupied'\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "model = load_model(top_model_weights_path)\n",
        "\n",
        "\n",
        "\n",
        "def make_prediction(image):\n",
        "    #Rescale image\n",
        "    img = image/255.\n",
        "\n",
        "    #Convert to a 4D tensor\n",
        "    image = np.expand_dims(img, axis=0)\n",
        "    #print(image.shape)\n",
        "\n",
        "    # make predictions on the preloaded model\n",
        "    class_predicted = model.predict(image)\n",
        "    inID = np.argmax(class_predicted[0])\n",
        "    label = class_dictionary[inID]\n",
        "    return label\n",
        "\n",
        "\n",
        "def predict_on_image(image, spot_dict = final_spot_dict, make_copy=True, color = [0, 255, 0], alpha=0.5):\n",
        "    if make_copy:\n",
        "        new_image = np.copy(image)\n",
        "        overlay = np.copy(image)\n",
        "    cnt_empty = 0\n",
        "    all_spots = 0\n",
        "    for spot in spot_dict.keys():\n",
        "        all_spots += 1\n",
        "        (x1, y1, x2, y2) = spot\n",
        "        (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))\n",
        "        #crop this image\n",
        "        spot_img = image[y1:y2, x1:x2]\n",
        "        spot_img = cv2.resize(spot_img, (48, 48))\n",
        "\n",
        "        label = make_prediction(spot_img)\n",
        "#         print(label)\n",
        "        if label == 'empty':\n",
        "            cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1)\n",
        "            cnt_empty += 1\n",
        "\n",
        "    cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image)\n",
        "\n",
        "    cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "    0.7, (255, 255, 255), 2)\n",
        "\n",
        "    cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "    0.7, (255, 255, 255), 2)\n",
        "    save = False\n",
        "\n",
        "    if save:\n",
        "        filename = 'with_marking.jpg'\n",
        "        cv2.imwrite(filename, new_image)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "\n",
        "predicted_images = list(map(predict_on_image, test_images))\n",
        "show_images(predicted_images)"
      ],
      "metadata": {
        "id": "xTkUZbBDcsZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbW4TF_GUl1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}